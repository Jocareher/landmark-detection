{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model import Dynamic_sparse_alignment_network\n",
    "\n",
    "from Config.default import _C as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing the image\n",
    "def preprocess_image(image):\n",
    "    resized_image = cv2.resize(image, (256, 256))\n",
    "    normalized_image = resized_image / 255.0\n",
    "    tensor_image = torch.tensor(normalized_image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "    return tensor_image\n",
    "\n",
    "# Extract 68 landmarks using the provided mapping\n",
    "def filter_68_landmarks(landmarks):\n",
    "    mapping_indices = [\n",
    "        0, 2, 5, 4, 7, 9, 12, 14, 16, 18, 20, 22, 25, 27, 29, 31, 32,\n",
    "        33, 34, 35, 36, 37, 42, 43, 44, 45, 46, 51, 52, 53, 54, 55,\n",
    "        56, 57, 58, 59, 60, 61, 63, 64, 65, 67, 68, 69, 71, 72, 73,\n",
    "        75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
    "        90, 91, 92, 93, 94, 95\n",
    "    ]\n",
    "    final_landmarks = landmarks[-1]  # Select the last stage\n",
    "    return final_landmarks[mapping_indices, :]\n",
    "\n",
    "# Plot the 68 landmarks on the image\n",
    "def draw_landmarks(image, landmarks):\n",
    "    h, w, _ = image.shape\n",
    "    diagonal = (w ** 2 + h ** 2) ** 0.5\n",
    "    radius = max(8, min(int(diagonal * 0.01), 16))  # Radius proportional to image diagonal\n",
    "    thickness = -1  # Filled circle\n",
    "\n",
    "    for (x, y) in landmarks:\n",
    "        x = int(x * w)\n",
    "        y = int(y * h)\n",
    "        cv2.circle(image, (x, y), radius, (0, 255, 0), thickness)\n",
    "    return image\n",
    "\n",
    "# Save landmarks to a .txt file\n",
    "def save_landmarks_to_txt(landmarks, output_txt_path, image_width, image_height):\n",
    "    landmarks[:, 0] *= image_width\n",
    "    landmarks[:, 1] *= image_height\n",
    "    landmarks = landmarks.flatten().astype(int)\n",
    "    with open(output_txt_path, 'w') as f:\n",
    "        f.write(\" \".join(map(str, landmarks)))  # Save in a single line\n",
    "\n",
    "# Process a single image and save results\n",
    "def process_image(image_path, output_image_dir, output_label_dir, model, device):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not load image: {image_path}\")\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    input_tensor = preprocess_image(image).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_list, _, _, _ = model(input_tensor)\n",
    "        landmarks = output_list[-1].squeeze().cpu().numpy()\n",
    "        landmarks_68 = filter_68_landmarks(landmarks)\n",
    "\n",
    "    # Save the plotted image\n",
    "    output_image_path = os.path.join(output_image_dir, os.path.basename(image_path))\n",
    "    plotted_image = draw_landmarks(image.copy(), landmarks_68)\n",
    "    cv2.imwrite(output_image_path, plotted_image)\n",
    "\n",
    "    # Save the landmarks in .txt format\n",
    "    output_label_path = os.path.join(output_label_dir, f\"{os.path.splitext(os.path.basename(image_path))[0]}.txt\")\n",
    "    save_landmarks_to_txt(landmarks_68, output_label_path, w, h)\n",
    "\n",
    "    return True\n",
    "\n",
    "# Process a directory of images\n",
    "def process_directory(input_dir, output_dir, model, device):\n",
    "    # Create output directories\n",
    "    image_output_dir = os.path.join(output_dir, 'images')\n",
    "    label_output_dir = os.path.join(output_dir, 'labels')\n",
    "    os.makedirs(image_output_dir, exist_ok=True)\n",
    "    os.makedirs(label_output_dir, exist_ok=True)\n",
    "\n",
    "    images = [f for f in os.listdir(input_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
    "    processed_count = 0\n",
    "    failed_images = []\n",
    "\n",
    "    print(\"\\nProcessing images...\")\n",
    "    for image_name in tqdm(images, desc=\"Processing\"):\n",
    "        image_path = os.path.join(input_dir, image_name)\n",
    "        try:\n",
    "            process_image(image_path, image_output_dir, label_output_dir, model, device)\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {image_name}: {e}\")\n",
    "            failed_images.append(image_name)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== Processing Summary ===\")\n",
    "    print(f\"Total images processed: {processed_count}\")\n",
    "    print(f\"Total images failed: {len(failed_images)}\")\n",
    "    if failed_images:\n",
    "        print(\"Failed images:\")\n",
    "        for img in failed_images:\n",
    "            print(f\"  {img}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11082/1825572725.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 311/311 [02:27<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Summary ===\n",
      "Total images processed: 311\n",
      "Total images failed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_directory = '/home/jocareher/Documents/baby_face_72/images'\n",
    "output_directory = '/home/jocareher/Documents/results_dslpt'\n",
    "\n",
    "# Load model\n",
    "model_path = '/home/jocareher/Downloads/DSLPT_WFLW_6_layers.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Dynamic_sparse_alignment_network(num_point=98, d_model=256, trainable=False,\n",
    "                                            return_interm_layers=False, nhead=8,\n",
    "                                            feedforward_dim=1024, initial_path='/home/jocareher/Downloads/DSLPT/Config/init_98.npz',\n",
    "                                            cfg=cfg)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Process directory\n",
    "process_directory(input_directory, output_directory, model, device)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
